{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous consecutive in a row unbroken\n",
    "# 6x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, time, math, csv\n",
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "from scipy import stats\n",
    "from csv import writer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf; import tensorflow.keras as tfk\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONST, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_TYPES = ['pose'] # ~\n",
    "# LANDMARK_TYPES = ['face', 'pose', 'left_hand', 'right_hand']\n",
    "\n",
    "num_pose_landmarks = 33\n",
    "num_face_landmarks = 468\n",
    "num_hand_landmarks = 21\n",
    "pose_landmark_num_vals = 4\n",
    "face_landmark_num_vals = 3\n",
    "hand_landmark_num_vals = 3\n",
    "\n",
    "num_features = 0\n",
    "if ('pose' in LANDMARK_TYPES):\n",
    "    num_features += num_pose_landmarks*pose_landmark_num_vals\n",
    "if ('face' in LANDMARK_TYPES):\n",
    "    num_features += num_face_landmarks*face_landmark_num_vals\n",
    "if ('left_hand' in LANDMARK_TYPES):\n",
    "    num_features += num_hand_landmarks*hand_landmark_num_vals\n",
    "if ('right_hand' in LANDMARK_TYPES):\n",
    "    num_features += num_hand_landmarks*hand_landmark_num_vals\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('DATA') \n",
    "\n",
    "# ACTIONS that we try to detect. ~\n",
    "ACTIONS = {\n",
    "    'PushUp': {'ANGLE': [70, 130], \n",
    "               'JOINTS': [['right_shoulder', 'right_elbow', 'right_wrist'],\n",
    "                          ['left_shoulder', 'left_elbow', 'left_wrist']]},\n",
    "    'Squat': {'ANGLE': [90, 160],\n",
    "              'JOINTS': [['right_hip', 'right_knee', 'right_ankle'],\n",
    "                         ['left_hip', 'left_knee', 'left_ankle']]},\n",
    "    'ReverseCrunch': {'ANGLE': [80, 110],\n",
    "                      'JOINTS': [['right_shoulder', 'right_hip', 'right_knee'],\n",
    "                                 ['left_shoulder', 'left_hip', 'left_knee']]},\n",
    "    'Lunge': {'ANGLE': [90, 160],\n",
    "              'JOINTS': [['right_hip', 'right_knee', 'right_ankle'],\n",
    "                         ['left_hip', 'left_knee', 'left_ankle']]},\n",
    "    'JumpingJacks': {'ANGLE': [30, 90],\n",
    "                     'JOINTS': [['right_elbow', 'right_shoulder', 'right_hip'],\n",
    "                                ['left_elbow', 'left_shoulder', 'left_hip']]},\n",
    "    'Standing': {'ANGLE': [None, None],\n",
    "                 'JOINTS': [[None, None, None],\n",
    "                            [None, None, None]]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image = cv2.flip(image, 1)\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results, LANDMARK_TYPES:list):\n",
    "    # _CONNECTIONS: Relationship between all of the different component\n",
    "    # within the detection set. Which landmarks between each set of models \n",
    "    # actually connect to each other\n",
    "    \n",
    "    if ('face' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks,\n",
    "                                mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    if ('pose' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks,\n",
    "                                mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    if ('left_hand' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks,\n",
    "                                mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    if ('right_hand' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks,\n",
    "                                mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_color():\n",
    "    color = []\n",
    "    for i in range(3):\n",
    "        channel = np.random.randint(0, 256)\n",
    "        color.append(channel)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_landmarks = {}\n",
    "if ('face' in LANDMARK_TYPES):\n",
    "    color_landmarks['face'] = {}\n",
    "if ('right_hand' in LANDMARK_TYPES):\n",
    "    color_landmarks['right_hand'] = {}\n",
    "if ('left_hand' in LANDMARK_TYPES):\n",
    "    color_landmarks['left_hand'] = {}\n",
    "if ('pose' in LANDMARK_TYPES):\n",
    "    color_landmarks['pose'] = {}\n",
    "    \n",
    "if ('face' in LANDMARK_TYPES):\n",
    "    color_landmarks['face']['keypoints'] = create_color()\n",
    "    color_landmarks['face']['connections'] = create_color()\n",
    "if ('right_hand' in LANDMARK_TYPES):\n",
    "    color_landmarks['right_hand']['keypoints'] = create_color()\n",
    "    color_landmarks['right_hand']['connections'] = create_color()\n",
    "if ('left_hand' in LANDMARK_TYPES):\n",
    "    color_landmarks['left_hand']['keypoints'] = create_color()\n",
    "    color_landmarks['left_hand']['connections'] = create_color()\n",
    "if ('pose' in LANDMARK_TYPES):\n",
    "    color_landmarks['pose']['keypoints'] = create_color()\n",
    "    color_landmarks['pose']['connections'] = create_color()\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if ('face' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_CONTOURS, \n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['face']['keypoints'], thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['face']['connections'], thickness=1, circle_radius=1))\n",
    "    if ('right_hand' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS, \n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['right_hand']['keypoints'], thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['right_hand']['connections'], thickness=2, circle_radius=2))\n",
    "    if ('left_hand' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS, \n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['left_hand']['keypoints'], thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['left_hand']['connections'], thickness=2, circle_radius=2))\n",
    "    if ('pose' in LANDMARK_TYPES):\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks,\n",
    "            mp_holistic.POSE_CONNECTIONS,   \n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['pose']['keypoints'], thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=color_landmarks['pose']['connections'], thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, \n",
    "#                           min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         ret, frame = cap.read()\n",
    "#         image, results = mediapipe_detection(frame, holistic) # Make detections\n",
    "#         draw_styled_landmarks(image, results) # Draw landmarks\n",
    "\n",
    "#         cv2.imshow('OpenCV Feed', image) # Show to screen\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "        \n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):    \n",
    "    keypoints = np.array([])\n",
    "    if ('pose' in LANDMARK_TYPES):\n",
    "      pose = np.array([[res.x, res.y, res.z, res.visibility] for res in \n",
    "                      results.pose_landmarks.landmark]).flatten() \\\n",
    "                        if results.pose_landmarks \\\n",
    "                        else np.zeros(num_pose_landmarks*pose_landmark_num_vals)\n",
    "      keypoints = np.concatenate([keypoints, pose])\n",
    "    if ('face' in LANDMARK_TYPES):\n",
    "      face = np.array([[res.x, res.y, res.z] for res in \n",
    "                      results.face_landmarks.landmark]).flatten() \\\n",
    "                        if results.face_landmarks \\\n",
    "                        else np.zeros(num_face_landmarks*face_landmark_num_vals)\n",
    "      keypoints = np.concatenate([keypoints, face])\n",
    "    if ('left_hand' in LANDMARK_TYPES):\n",
    "      left_hand = np.array([[res.x, res.y, res.z] for res in \n",
    "                            results.left_hand_landmarks.landmark]).flatten() \\\n",
    "                              if results.left_hand_landmarks \\\n",
    "                              else np.zeros(num_hand_landmarks*hand_landmark_num_vals)\n",
    "      keypoints = np.concatenate([keypoints, left_hand])\n",
    "    if ('right_hand' in LANDMARK_TYPES):\n",
    "      right_hand = np.array([[res.x, res.y, res.z] for res in\n",
    "                            results.right_hand_landmarks.landmark]).flatten() \\\n",
    "                              if results.right_hand_landmarks \\\n",
    "                              else np.zeros(num_hand_landmarks*hand_landmark_num_vals)\n",
    "      keypoints = np.concatenate([keypoints, right_hand])\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = {}\n",
    "for action in ACTIONS.keys():\n",
    "    file_names[action] = os.path.join(DATA_PATH, f'{action}.csv')\n",
    "\n",
    "DATA_PATH = os.path.join('DATA')\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    \n",
    "label_map = {label:num for num, label in enumerate(list(ACTIONS.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create csv file based file_names\n",
    "# for file_name in file_names.values():\n",
    "#     with open(file_name, 'w', newline='') as f:\n",
    "#         f.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def append_row_to_file(file_name, row):\n",
    "    with open(file_name, 'a+', newline='') as f:\n",
    "        # Write new line data to csv file\n",
    "        writer_object = writer(f)\n",
    "        writer_object.writerow(row)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_one_action(cap, file_name):\n",
    "\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, \n",
    "                            min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        while cap.isOpened():            \n",
    "            try:\n",
    "                ret, frame = cap.read() # Read feed\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                image, results = mediapipe_detection(frame, holistic) # Make detections\n",
    "                draw_styled_landmarks(image, results) # Draw landmarks\n",
    "                append_row_to_file(file_name, extract_keypoints(results))\n",
    "                \n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PushUp', 'Squat', 'ReverseCrunch', 'Lunge', 'JumpingJacks', 'Standing']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ACTIONS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(r'Videos\\Standing.mp4')\n",
    "# collect_data_one_action(cap=cap, file_name=file_names['Standing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('REMOVE ZERO ROWS', '-'*50)\n",
    "# for action in list(ACTIONS.keys()):\n",
    "#     with open(file_names[action], 'r') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         data = list(reader)\n",
    "#         # Remove '' in data\n",
    "#         data = [[x for x in data_i if (x != '')] for data_i in data if (data_i != [])]\n",
    "#         data = np.array([x for x in data if (x != [])], dtype=np.float32)\n",
    "#         print(f'{action}: {data.shape} -> ', end='')\n",
    "        \n",
    "#         i = 0\n",
    "#         for data_i in data:\n",
    "#             if (sum(data_i) == 0):\n",
    "#                 # Remove rows with all zeros in the csv file\n",
    "#                 # This is to remove the rows where no landmarks were detected\n",
    "#                 # and the array was filled with zeros\n",
    "#                 data = np.delete(data, i, 0)\n",
    "#                 # Decrement i to account for the deleted row so that the next row is not skipped\n",
    "#                 # when the for loop increments i\n",
    "#                 # This is because the array is now one row shorter and the next row is now the current row\n",
    "#                 # (i.e. the row that was skipped)\n",
    "#                 i -= 1\n",
    "#             i += 1\n",
    "#         print(data.shape)\n",
    "        \n",
    "#         # Delete and create new csv file\n",
    "#         # This is to remove the rows where no landmarks were detected\n",
    "#         # and the array was filled with zeros\n",
    "#         # This is done so that the data is not used in training\n",
    "#         # and the model does not learn to predict zeros\n",
    "#         # (i.e. the model does not learn to predict no landmarks)\n",
    "#         # This is done because the model will be used in real-time\n",
    "#         # and it is better to have the model predict no landmarks\n",
    "#         # than to have the model predict landmarks that are not there\n",
    "#     os.remove(file_names[action])\n",
    "        \n",
    "#     with open(file_names[action], 'w', newline='') as f:\n",
    "#         writer_object = writer(f)\n",
    "#         for row in data:\n",
    "#             writer_object.writerow(row)\n",
    "\n",
    "# print('BALANCING CLASSES', '-'*50)\n",
    "# num_rows = {}\n",
    "# for action in list(ACTIONS.keys()):\n",
    "#     with open(file_names[action], 'r') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         data = list(reader)\n",
    "#         data = np.array(data, dtype=np.float32)\n",
    "        \n",
    "#         row = data.shape[0]\n",
    "#         num_rows[action] = row\n",
    "# num_row_min = min(num_rows.values())\n",
    "\n",
    "# for action in list(ACTIONS.keys()):\n",
    "#     with open(file_names[action], 'r') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         data = list(reader)\n",
    "#         data = np.array(data, dtype=np.float32)\n",
    "#         print(f'{action}: {data.shape} -> ', end='')\n",
    "#         data = data[:num_row_min]\n",
    "    \n",
    "#     os.remove(file_names[action])\n",
    "        \n",
    "#     with open(file_names[action], 'w', newline='') as f:\n",
    "#         writer_object = writer(f)\n",
    "#         for row in data:\n",
    "#             writer_object.writerow(row)\n",
    "#         print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Video are going to be SEQUENCE_LENGTH frames (each frame contains a no. extracted \n",
    "# keypoint) in length. ~\n",
    "SEQUENCE_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map: {'PushUp': 0, 'Squat': 1, 'ReverseCrunch': 2, 'Lunge': 3, 'JumpingJacks': 4, 'Standing': 5}\n",
      "Sequences shape: (2310, 60, 132)\n",
      "Labels shape: (2310,)\n",
      "X_train shape: (2194, 60, 132)\n",
      "y_train shape: (2194, 6)\n",
      "X_test shape: (116, 60, 132)\n",
      "y_test shape: (116, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequences, labels = [], []\n",
    "# Extract content of csv file into sequences\n",
    "for action in list(ACTIONS.keys()):\n",
    "    with open(file_names[action], 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        data = list(reader)\n",
    "        \n",
    "        for i in range(0, len(data) - SEQUENCE_LENGTH, SEQUENCE_LENGTH):\n",
    "            # Extract SEQUENCE_LENGTH frames from csv file\n",
    "            sequence = data[i: i + SEQUENCE_LENGTH]\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences, dtype=np.float32)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "print(f'label_map: {label_map}')\n",
    "print(f'Sequences shape: {np.array(sequences).shape}')\n",
    "print(f'Labels shape: {np.array(labels).shape}')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 64)            50432     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 60, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205,094\n",
      "Trainable params: 205,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "checkpoint_filepath = os.path.join('checkpoint')\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True,\n",
    "    monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 1e-4\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr*math.pow(drop, math.floor((1 + epoch)/epochs_drop))\n",
    "    return lr\n",
    "LRScheduler_callback = tfk.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "reduceLROnPlateau_callback = tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                factor=0.2, verbose=1, patience=1, min_lr=0.001)\n",
    "\n",
    "callback_earlyStopping = tfk.callbacks.EarlyStopping(patience=100, min_delta=0.05,\n",
    "                            baseline=0.8, mode='min', monitor='val_loss',\n",
    "                            restore_best_weights=True, verbose=1)\n",
    "\n",
    "model = tfk.models.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, activation=nn.leaky_relu, \n",
    "                input_shape=(SEQUENCE_LENGTH, num_features)),\n",
    "    layers.LSTM(128, return_sequences=True, activation=nn.leaky_relu),\n",
    "    layers.LSTM(64, return_sequences=False, activation=nn.leaky_relu),\n",
    "    layers.Dense(64, activation=nn.leaky_relu),\n",
    "    layers.Dense(32, activation=nn.leaky_relu),\n",
    "    layers.Dense(len(list(ACTIONS.keys())), activation=nn.softmax)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tfk.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tfk.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/40\n",
      "69/69 [==============================] - 13s 130ms/step - loss: 1.5538 - accuracy: 0.3090 - val_loss: 1.5123 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 2/40\n",
      "69/69 [==============================] - 8s 114ms/step - loss: 0.9932 - accuracy: 0.6600 - val_loss: 1.0638 - val_accuracy: 0.6983 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 3/40\n",
      "69/69 [==============================] - 8s 114ms/step - loss: 0.7829 - accuracy: 0.7179 - val_loss: 0.6358 - val_accuracy: 0.7672 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 4/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.6579 - accuracy: 0.7493 - val_loss: 0.7878 - val_accuracy: 0.6897 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 5/40\n",
      "69/69 [==============================] - 8s 113ms/step - loss: 0.5955 - accuracy: 0.7671 - val_loss: 0.7728 - val_accuracy: 0.6810 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 6/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.5756 - accuracy: 0.7876 - val_loss: 0.7072 - val_accuracy: 0.7931 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 7/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.5172 - accuracy: 0.8058 - val_loss: 0.5164 - val_accuracy: 0.7931 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 8/40\n",
      "69/69 [==============================] - 8s 117ms/step - loss: 0.5232 - accuracy: 0.7981 - val_loss: 0.7154 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 9/40\n",
      "69/69 [==============================] - 8s 114ms/step - loss: 0.5196 - accuracy: 0.8022 - val_loss: 0.6996 - val_accuracy: 0.8534 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 10/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.4236 - accuracy: 0.8432 - val_loss: 0.5874 - val_accuracy: 0.8362 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 11/40\n",
      "69/69 [==============================] - 8s 114ms/step - loss: 0.3728 - accuracy: 0.8532 - val_loss: 0.3159 - val_accuracy: 0.8621 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 12/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.3686 - accuracy: 0.8624 - val_loss: 0.2909 - val_accuracy: 0.8966 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 13/40\n",
      "69/69 [==============================] - 8s 118ms/step - loss: 0.3473 - accuracy: 0.8710 - val_loss: 0.2685 - val_accuracy: 0.8966 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 14/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.3295 - accuracy: 0.8710 - val_loss: 0.2684 - val_accuracy: 0.9052 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 15/40\n",
      "69/69 [==============================] - 8s 113ms/step - loss: 0.3389 - accuracy: 0.8669 - val_loss: 0.3808 - val_accuracy: 0.8448 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 16/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 1.0849 - accuracy: 0.7388 - val_loss: 1.0232 - val_accuracy: 0.5862 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 17/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.9509 - accuracy: 0.6016 - val_loss: 0.8260 - val_accuracy: 0.6207 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 18/40\n",
      "69/69 [==============================] - 8s 118ms/step - loss: 0.7604 - accuracy: 0.6996 - val_loss: 0.7556 - val_accuracy: 0.7155 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 19/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.6629 - accuracy: 0.7593 - val_loss: 0.6478 - val_accuracy: 0.7328 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 20/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.5932 - accuracy: 0.8086 - val_loss: 0.6366 - val_accuracy: 0.7759 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 21/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.5545 - accuracy: 0.8168 - val_loss: 0.5941 - val_accuracy: 0.7759 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 22/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.5201 - accuracy: 0.8213 - val_loss: 0.5513 - val_accuracy: 0.7931 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 23/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.4926 - accuracy: 0.8195 - val_loss: 0.5051 - val_accuracy: 0.7845 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 24/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.4644 - accuracy: 0.8291 - val_loss: 0.4705 - val_accuracy: 0.8017 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 25/40\n",
      "69/69 [==============================] - 8s 118ms/step - loss: 0.4456 - accuracy: 0.8382 - val_loss: 0.4521 - val_accuracy: 0.8017 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 26/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.4258 - accuracy: 0.8446 - val_loss: 0.4364 - val_accuracy: 0.8190 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 27/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.4098 - accuracy: 0.8551 - val_loss: 0.4090 - val_accuracy: 0.8190 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 28/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.4051 - accuracy: 0.8505 - val_loss: 0.3780 - val_accuracy: 0.8621 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 29/40\n",
      "69/69 [==============================] - 8s 118ms/step - loss: 0.3933 - accuracy: 0.8578 - val_loss: 0.3978 - val_accuracy: 0.8362 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 30/40\n",
      "69/69 [==============================] - 8s 114ms/step - loss: 0.3748 - accuracy: 0.8692 - val_loss: 0.3624 - val_accuracy: 0.8879 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 31/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.3693 - accuracy: 0.8692 - val_loss: 0.3563 - val_accuracy: 0.8707 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 32/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.3652 - accuracy: 0.8742 - val_loss: 0.3490 - val_accuracy: 0.8793 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 33/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.3596 - accuracy: 0.8765 - val_loss: 0.3480 - val_accuracy: 0.8621 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 34/40\n",
      "69/69 [==============================] - 8s 117ms/step - loss: 0.3556 - accuracy: 0.8783 - val_loss: 0.3567 - val_accuracy: 0.8621 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 35/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.3505 - accuracy: 0.8783 - val_loss: 0.3456 - val_accuracy: 0.8621 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 36/40\n",
      "69/69 [==============================] - 8s 117ms/step - loss: 0.3468 - accuracy: 0.8820 - val_loss: 0.3402 - val_accuracy: 0.8621 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 37/40\n",
      "69/69 [==============================] - 8s 115ms/step - loss: 0.3471 - accuracy: 0.8806 - val_loss: 0.3649 - val_accuracy: 0.8534 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 38/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.3410 - accuracy: 0.8810 - val_loss: 0.3391 - val_accuracy: 0.8621 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1.25e-05.\n",
      "Epoch 39/40\n",
      "69/69 [==============================] - 8s 116ms/step - loss: 0.3384 - accuracy: 0.8810 - val_loss: 0.3425 - val_accuracy: 0.8707 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 6.25e-06.\n",
      "Epoch 40/40\n",
      "69/69 [==============================] - 8s 117ms/step - loss: 0.3326 - accuracy: 0.8865 - val_loss: 0.3447 - val_accuracy: 0.8621 - lr: 6.2500e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b20d9f3b20>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test), # 40\n",
    "          callbacks=[callback_earlyStopping, LRScheduler_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict(X_test)\n",
    "# print(f'y_pred: {list(ACTIONS.keys())[np.argmax(res[4])]}')\n",
    "# print(f'y_true: {list(ACTIONS.keys())[np.argmax(y_test[4])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('action.h5')\n",
    "# model.save('action1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model.predict(X_test)\n",
    "# ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for i in range(len(ACTIONS)):\n",
    "    colors.append(create_color())\n",
    "def prob_viz(res_prob, ACTIONS, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    for i, prob in enumerate(res_prob):\n",
    "        cv2.rectangle(output_frame, (0, 60 + i*40), (int(prob*100), 90 + i*40),\n",
    "                      create_color(), -1)\n",
    "        cv2.putText(output_frame, ACTIONS[i], (0, 85 + i*40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, colors[i], 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >\n",
    "\n",
    "https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_LANDMARKS = {'nose': 0,\n",
    "                  'left_eye_inner': 1, 'left_eye': 2, 'left_eye_outer': 3,\n",
    "                  'right_eye_inner': 4, 'right_eye': 5, 'right_eye_outer': 6,\n",
    "                  'left_ear': 7, 'right_ear': 8,\n",
    "                  'mouth_left': 9, 'mouth_right': 10,\n",
    "                  'left_shoulder': 11, 'right_shoulder': 12,\n",
    "                  'left_elbow': 13, 'right_elbow': 14,\n",
    "                  'left_wrist': 15, 'right_wrist': 16,\n",
    "                  'left_pinky': 17, 'right_pinky': 18,\n",
    "                  'left_index': 19, 'right_index': 20,\n",
    "                  'left_thumb': 21, 'right_thumb': 22,\n",
    "                  'left_hip': 23, 'right_hip': 24,\n",
    "                  'left_knee': 25, 'right_knee': 26,\n",
    "                  'left_ankle': 27, 'right_ankle': 28,\n",
    "                  'left_heel': 29, 'right_heel': 30,\n",
    "                  'left_foot_index': 31, 'right_foot_index': 32}\n",
    "\n",
    "def get_joint_coordinate(results, joint_name, get_z=False):\n",
    "    joint = results.pose_landmarks.landmark[POSE_LANDMARKS[joint_name]]\n",
    "    if get_z:\n",
    "        return [joint.x, joint.y, joint.z]\n",
    "    else:\n",
    "        return [joint.x, joint.y]\n",
    "    \n",
    "def get_three_joint_coordinates(results, joint_names, get_z=False):\n",
    "    joint_coordinates = []\n",
    "    for joint_name in joint_names:\n",
    "        joint_coordinates.append(get_joint_coordinate(results, joint_name, get_z))\n",
    "    return joint_coordinates\n",
    "\n",
    "def get_three_joint_coordinates_two_sides(results, joint_names_left,\n",
    "                            joint_names_right, get_z=False):\n",
    "    joint_coordinates_left = get_three_joint_coordinates(results, joint_names_left, get_z)\n",
    "    joint_coordinates_right = get_three_joint_coordinates(results, joint_names_right, get_z)\n",
    "    return [joint_coordinates_left, joint_coordinates_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(joint_coordinates):\n",
    "    pt1 = np.array(joint_coordinates[0]) # First\n",
    "    pt2 = np.array(joint_coordinates[1]) # Mid\n",
    "    pt3 = np.array(joint_coordinates[2]) # End\n",
    "    \n",
    "    # x: 0, y: 1\n",
    "    # arctan2 measures the counterclockwise angle θ, in radians, between the positive \n",
    "    # x-axis and the point (x, y)\n",
    "    angle_radians = np.arctan2(pt3[1] - pt2[1], pt3[0] - pt2[0]) - \\\n",
    "                    np.arctan2(pt1[1] - pt2[1], pt1[0]- pt2[0])\n",
    "    angle_degree = np.abs(angle_radians*180.0/np.pi)\n",
    "    \n",
    "    if angle_degree > 180.0: # Convert to 0-180 degree\n",
    "        angle_degree = 360 - angle_degree\n",
    "        \n",
    "    return angle_degree\n",
    "\n",
    "def calculate_angle_two_side(joint_coordinates_left, joint_coordinates_right):\n",
    "    angle_left = calculate_angle(joint_coordinates_left)\n",
    "    angle_right = calculate_angle(joint_coordinates_right)\n",
    "    \n",
    "    return angle_left, angle_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting(angle, stage, counter, max_angle, min_angle):\n",
    "    if angle[0] > max_angle:\n",
    "        stage[0] = \"down\"\n",
    "    if angle[0] < min_angle and stage[0] == 'down':\n",
    "        stage[0] = \"up\"\n",
    "        counter[0] += 1\n",
    "    if angle[1] > max_angle:\n",
    "        stage[1] = \"down\"\n",
    "    if angle[1] < min_angle and stage[1] == 'down':\n",
    "        stage[1] = \"up\"\n",
    "        counter[1] += 1\n",
    "    return stage, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM_DIM = [640, 480]\n",
    "COLOR_REPS = create_color()\n",
    "COlOR_STAGE = create_color()\n",
    "COLOR_ANGLE = create_color()\n",
    "COLOR_ACTION = create_color()\n",
    "\n",
    "def show_status_box(image, counter, stage, angle, mid_point, action, is_two_side=False):\n",
    "    # Render curl counter   \n",
    "    # Setup status box\n",
    "    BOX_WIDTH = 120\n",
    "    BOX_HEIGHT = 35\n",
    "    BOX_COLOR = (255, 255, 255)\n",
    "    \n",
    "    TEXT_REPS_START_LEFT = [10, 12]\n",
    "    TEXT_STAGE_START_LEFT = [65, 12]\n",
    "    TEXT_NUM_REPS_START_LEFT = [TEXT_REPS_START_LEFT[0], 30]\n",
    "    TEXT_NUM_STAGE_START_LEFT = [TEXT_STAGE_START_LEFT[0], 30]\n",
    "    \n",
    "    TEXT_REPS_START_RIGHT = [WEBCAM_DIM[0] - BOX_WIDTH + TEXT_REPS_START_LEFT[0], \n",
    "                             TEXT_REPS_START_LEFT[1]]\n",
    "    TEXT_STAGE_START_RIGHT = [WEBCAM_DIM[0] - BOX_WIDTH + TEXT_STAGE_START_LEFT[0], \n",
    "                              TEXT_STAGE_START_LEFT[1]]\n",
    "    TEXT_NUM_REPS_START_RIGHT = [WEBCAM_DIM[0] - BOX_WIDTH + TEXT_REPS_START_LEFT[0],\n",
    "                                 TEXT_NUM_REPS_START_LEFT[1]]\n",
    "    TEXT_NUM_STAGE_START_RIGHT = [WEBCAM_DIM[0] - BOX_WIDTH + TEXT_STAGE_START_LEFT[0],\n",
    "                                  TEXT_NUM_REPS_START_LEFT[1]]\n",
    "    \n",
    "    cv2.rectangle(image, (0, 0), (WEBCAM_DIM[0], BOX_HEIGHT), BOX_COLOR, -1, )\n",
    "    cv2.putText(image, 'REPS', TEXT_REPS_START_LEFT, \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_REPS, 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, 'STAGE', TEXT_STAGE_START_LEFT, \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COlOR_STAGE, 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, 'REPS', TEXT_REPS_START_RIGHT, \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_REPS, 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, 'STAGE', TEXT_STAGE_START_RIGHT, \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, COlOR_STAGE, 2, cv2.LINE_AA)\n",
    "    \n",
    "    BOX_ACTION_WIDTH = 75\n",
    "    cv2.putText(image, 'ACTION', (int(WEBCAM_DIM[0]//2) - 25, 12), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_ACTION, 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, action, (int(WEBCAM_DIM[0]//2 - BOX_ACTION_WIDTH), 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_ACTION, 2, cv2.LINE_AA)\n",
    "    \n",
    "    ANGLE_TEXT_SIZE = 2\n",
    "    \n",
    "    if is_two_side:\n",
    "        cv2.putText(image, str(counter[0]), TEXT_NUM_REPS_START_LEFT, \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_REPS, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(stage[0]), TEXT_NUM_STAGE_START_LEFT, \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COlOR_STAGE, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(int(angle[0])),\n",
    "                    tuple(np.multiply(mid_point[0], WEBCAM_DIM).astype(int)), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, ANGLE_TEXT_SIZE, COLOR_ANGLE, 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(image, str(counter[1]), TEXT_NUM_REPS_START_RIGHT, \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_REPS, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(stage[1]), TEXT_NUM_STAGE_START_RIGHT, \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COlOR_STAGE, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(int(angle[1])),\n",
    "                    tuple(np.multiply(mid_point[1], WEBCAM_DIM).astype(int)), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, ANGLE_TEXT_SIZE, COLOR_ANGLE, 2, cv2.LINE_AA)\n",
    "    else:  \n",
    "        counter = counter[0]\n",
    "        stage = stage[0]\n",
    "        angle = angle[0]\n",
    "        mid_point = mid_point[0]\n",
    "        \n",
    "        cv2.putText(image, str(counter), TEXT_NUM_REPS_START_LEFT, \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_REPS, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, TEXT_NUM_STAGE_START_LEFT, \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COlOR_STAGE, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(int(angle)),\n",
    "                    tuple(np.multiply(mid_point, WEBCAM_DIM).astype(int)), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, ANGLE_TEXT_SIZE, COLOR_ANGLE, 2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curl counter variables\n",
    "counter = [0, 0]\n",
    "stage = [None, None]\n",
    "angle = [0, 0]\n",
    "mid_point = [None, None]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic) # Make detections\n",
    "        draw_styled_landmarks(image, results) # Draw landmarks\n",
    "\n",
    "        action = 'Squat'         \n",
    "        action_angle = ACTIONS[action]['ANGLE']            \n",
    "        joint_names_left, joint_names_right = ACTIONS[action]['JOINTS']\n",
    "        \n",
    "        # Extract landmarks (Joint, coordinate). Determining Joints\n",
    "        try:  \n",
    "            joint_coordinates_left, joint_coordinates_right = get_three_joint_coordinates_two_sides(results, joint_names_left,\n",
    "                            joint_names_right, get_z=False)\n",
    "            mid_point = [joint_coordinates_left[1], joint_coordinates_right[1]]\n",
    "            angle = calculate_angle_two_side(joint_coordinates_left, joint_coordinates_right)\n",
    "            stage, counter = counting(angle, stage, counter,\n",
    "                                      min_angle=action_angle[0], \n",
    "                                      max_angle=action_angle[1])\n",
    "            show_status_box(image, counter, stage, angle, mid_point, action, \n",
    "                            is_two_side=True)       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SURE_LIMIT = 5\n",
    "MOST_OCCURRING_ACTION_LIMIT = ACTION_SURE_LIMIT//2\n",
    "MAX_REPS = 5\n",
    "\n",
    "def do_counting(cap, action, max_reps):\n",
    "    # Curl counter variables\n",
    "    counter = [0, 0]\n",
    "    stage = [None, None]\n",
    "    angle = [0, 0]\n",
    "    mid_point = [None, None]\n",
    "    cv2.destroyWindow('Detection')\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic) # Make detections\n",
    "            draw_styled_landmarks(image, results) # Draw landmarks\n",
    "\n",
    "            action_angle = ACTIONS[action]['ANGLE']            \n",
    "            joint_names_left, joint_names_right = ACTIONS[action]['JOINTS']\n",
    "            \n",
    "            # Extract landmarks (Joint, coordinate). Determining Joints\n",
    "            try:  \n",
    "                joint_coordinates_left, joint_coordinates_right = get_three_joint_coordinates_two_sides(results, joint_names_left,\n",
    "                                joint_names_right, get_z=False)\n",
    "                mid_point = [joint_coordinates_left[1], joint_coordinates_right[1]]\n",
    "                angle = calculate_angle_two_side(joint_coordinates_left, joint_coordinates_right)\n",
    "                stage, counter = counting(angle, stage, counter,\n",
    "                                        min_angle=action_angle[0], \n",
    "                                        max_angle=action_angle[1])\n",
    "                show_status_box(image, counter, stage, angle, mid_point, action, \n",
    "                                is_two_side=True)     \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            cv2.imshow('Counting', image)\n",
    "\n",
    "            if (counter[0] > max_reps - 1 or counter[1] > max_reps - 1):\n",
    "                do_detection(action_sure_limit=ACTION_SURE_LIMIT, SEQUENCE_LENGTH=SEQUENCE_LENGTH, model=model,\n",
    "                             is_detection=False)\n",
    "            # else:\n",
    "            #     cap.release()\n",
    "            #     cv2.destroyAllWindows()\n",
    "            \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "# do_counting(cap, action='Squat', max_reps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 674ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m actions\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m actions \u001b[39m=\u001b[39m do_detection(action_sure_limit\u001b[39m=\u001b[39;49mACTION_SURE_LIMIT, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                        SEQUENCE_LENGTH\u001b[39m=\u001b[39;49mSEQUENCE_LENGTH, model\u001b[39m=\u001b[39;49mmodel, is_detection\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36mdo_detection\u001b[1;34m(action_sure_limit, SEQUENCE_LENGTH, model, is_detection)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mif\u001b[39;00m (todo_count \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         do_counting(cap, action\u001b[39m=\u001b[39;49mres_label, max_reps\u001b[39m=\u001b[39;49mMAX_REPS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36mdo_counting\u001b[1;34m(cap, action, max_reps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mCounting\u001b[39m\u001b[39m'\u001b[39m, image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m (counter[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m max_reps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m counter[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m max_reps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     do_detection(action_sure_limit\u001b[39m=\u001b[39;49mACTION_SURE_LIMIT, SEQUENCE_LENGTH\u001b[39m=\u001b[39;49mSEQUENCE_LENGTH, model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                  is_detection\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#     cap.release()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#     cv2.destroyAllWindows()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m10\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36mdo_detection\u001b[1;34m(action_sure_limit, SEQUENCE_LENGTH, model, is_detection)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mif\u001b[39;00m (todo_count \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         do_counting(cap, action\u001b[39m=\u001b[39;49mres_label, max_reps\u001b[39m=\u001b[39;49mMAX_REPS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36mdo_counting\u001b[1;34m(cap, action, max_reps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m cap\u001b[39m.\u001b[39misOpened():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     image, results \u001b[39m=\u001b[39m mediapipe_detection(frame, holistic) \u001b[39m# Make detections\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     draw_styled_landmarks(image, results) \u001b[39m# Draw landmarks\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     action_angle \u001b[39m=\u001b[39m ACTIONS[action][\u001b[39m'\u001b[39m\u001b[39mANGLE\u001b[39m\u001b[39m'\u001b[39m]            \n",
      "\u001b[1;32mc:\\Users\\acer\\OneDrive\\SPECIALIZATION\\AI\\AI_PROJECTS\\POSE ESTIMATION - ACTION RECOGNITION\\05_Sign Language Detection_ACTION RECOGNITION_LSTM\\Action Detection Refined.ipynb Cell 52\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmediapipe_detection\u001b[39m(image, model):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(image, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB) \u001b[39m# COLOR CONVERSION BGR 2 RGB\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mflip(image, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/OneDrive/SPECIALIZATION/AI/AI_PROJECTS/POSE%20ESTIMATION%20-%20ACTION%20RECOGNITION/05_Sign%20Language%20Detection_ACTION%20RECOGNITION_LSTM/Action%20Detection%20Refined.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m                  \u001b[39m# Image is no longer writeable\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def do_detection(action_sure_limit, SEQUENCE_LENGTH, model, is_detection=True):\n",
    "    if is_detection == False:\n",
    "        cv2.destroyWindow('Counting')\n",
    "    \n",
    "    # Detection variables\n",
    "    sequences = []\n",
    "    actions = []\n",
    "    threshold = 0.7\n",
    "    todo_count = False\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read() # Read feed\n",
    "            image, results = mediapipe_detection(frame, holistic) # Make detections\n",
    "            draw_styled_landmarks(image, results) # Draw landmarks\n",
    "            \n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequences.append(keypoints)\n",
    "            sequences = sequences[-SEQUENCE_LENGTH:]\n",
    "            \n",
    "            if len(sequences) == SEQUENCE_LENGTH:\n",
    "                res_prob = model.predict(np.expand_dims(sequences, axis=0))[0]\n",
    "                res_max_prob_idx = np.argmax(res_prob)\n",
    "                res_label = list(ACTIONS.keys())[res_max_prob_idx]\n",
    "                \n",
    "                actions.append(res_label)\n",
    "                \n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res_prob, list(ACTIONS.keys()), image) \n",
    "                \n",
    "                # Check if actions are the same for the last `action_sure_limit` frames\n",
    "                # If so, then we are sure of the action. If not, then we are not sure of the action\n",
    "                if (actions == list(np.full((action_sure_limit,), res_label))):\n",
    "                    todo_count = True\n",
    "                    \n",
    "            actions = actions[-action_sure_limit:]\n",
    "            cv2.imshow('Detection', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "            if (todo_count == True):\n",
    "                do_counting(cap, action=res_label, max_reps=MAX_REPS)\n",
    "                break\n",
    "            \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return actions\n",
    "\n",
    "actions = do_detection(action_sure_limit=ACTION_SURE_LIMIT, \n",
    "                       SEQUENCE_LENGTH=SEQUENCE_LENGTH, model=model, is_detection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PushUp', 'PushUp', 'PushUp', 'PushUp', 'PushUp', 'PushUp', 'PushUp']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "229cdfb8eedfa4964725b7eb0da8d7a63b25d97a6ab808f09bd6b506844c0629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
